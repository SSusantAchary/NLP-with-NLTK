{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "What is Lemmatization ?\r\n",
    "\r\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\r\n",
    "Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\r\n",
    "\r\n",
    "Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. \r\n",
    "Some treat these two as same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological\r\n",
    "analysis of the words.\r\n",
    "\r\n",
    "Applications of lemmatization are:\r\n",
    "\r\n",
    "-Used in comprehensive retrieval systems like search engines.\r\n",
    "-Used in compact indexing\r\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "example_words = [\"word\", \"wordy\", \"wording\", \"cacti\", \"rocks\", \"catty\", \"demonic\", \"geese\", \"ravishing\", \"better\", \"best\", \"run\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "for w in example_words:\r\n",
    "    print(lemmatizer.lemmatize(w, pos=\"a\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word\n",
      "wordy\n",
      "wording\n",
      "cacti\n",
      "rocks\n",
      "catty\n",
      "demonic\n",
      "geese\n",
      "ravishing\n",
      "good\n",
      "best\n",
      "run\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for w in example_words:\r\n",
    "    print(lemmatizer.lemmatize(w, pos=\"v\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word\n",
      "wordy\n",
      "word\n",
      "cacti\n",
      "rock\n",
      "catty\n",
      "demonic\n",
      "geese\n",
      "ravish\n",
      "better\n",
      "best\n",
      "run\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for w in example_words:\r\n",
    "    print(lemmatizer.lemmatize(w))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word\n",
      "wordy\n",
      "wording\n",
      "cactus\n",
      "rock\n",
      "catty\n",
      "demonic\n",
      "goose\n",
      "ravishing\n",
      "better\n",
      "best\n",
      "run\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Additional\r\n",
    "# i) WordNetLemmatizer() without POS tag"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "\r\n",
    "text = \"She jumped into the river and breathed heavily\"\r\n",
    "wordnet = WordNetLemmatizer()\r\n",
    "tokenizer = word_tokenize(text)\r\n",
    "\r\n",
    "for token in tokenizer:\r\n",
    "    print(token,\"--->\",wordnet.lemmatize(token))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "She ---> She\n",
      "jumped ---> jumped\n",
      "into ---> into\n",
      "the ---> the\n",
      "river ---> river\n",
      "and ---> and\n",
      "breathed ---> breathed\n",
      "heavily ---> heavily\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "\r\n",
    "text = \"I am running and I usually use to runs\"\r\n",
    "\r\n",
    "wordnet = WordNetLemmatizer()\r\n",
    "tokenizer = word_tokenize(text)\r\n",
    "\r\n",
    "for token in tokenizer:\r\n",
    "    print(token,\"--->\",wordnet.lemmatize(token))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I ---> I\n",
      "am ---> am\n",
      "running ---> running\n",
      "and ---> and\n",
      "I ---> I\n",
      "usually ---> usually\n",
      "use ---> use\n",
      "to ---> to\n",
      "runs ---> run\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#ii) WordNetLemmatizer() with POS tags"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk import word_tokenize,pos_tag\r\n",
    "\r\n",
    "text = \"She jumped into the river and breathed heavily\"\r\n",
    "wordnet = WordNetLemmatizer()\r\n",
    "\r\n",
    "for token,tag in pos_tag(word_tokenize(text)):\r\n",
    "    pos=tag[0].lower()\r\n",
    "        \r\n",
    "    if pos not in ['a', 'r', 'n', 'v']:\r\n",
    "        pos='n'\r\n",
    "    \r\n",
    "    print(token,\"--->\",wordnet.lemmatize(token,pos))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "She ---> She\n",
      "jumped ---> jump\n",
      "into ---> into\n",
      "the ---> the\n",
      "river ---> river\n",
      "and ---> and\n",
      "breathed ---> breathe\n",
      "heavily ---> heavily\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk import word_tokenize,pos_tag\r\n",
    "\r\n",
    "text = \"I am running and I usually use to runs\"\r\n",
    "wordnet = WordNetLemmatizer()\r\n",
    "\r\n",
    "for token,tag in pos_tag(word_tokenize(text)):\r\n",
    "    pos=tag[0].lower()\r\n",
    "        \r\n",
    "    if pos not in ['a', 'r', 'n', 'v']:\r\n",
    "        pos='n'\r\n",
    "    \r\n",
    "    print(token,\"--->\",wordnet.lemmatize(token,pos))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I ---> I\n",
      "am ---> be\n",
      "running ---> run\n",
      "and ---> and\n",
      "I ---> I\n",
      "usually ---> usually\n",
      "use ---> use\n",
      "to ---> to\n",
      "runs ---> run\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "Stemming vs Lemmatization\r\n",
    "Although both look quite similar there are key differences between Stemming vs Lemmatization –\r\n",
    "\r\n",
    "-The output of lemmatization is an actual word like Changing -> Change\r\n",
    "but stemming may not produce an actual English word like Changing -> Chang.\r\n",
    "\r\n",
    "-The stemming process just follows the step-by-step implementation of algorithms like SnowBall, Porter, etc.\r\n",
    "to derive the stem. Whereas lemmatization makes use of a lookup database like WordNet to derive lemma. \r\n",
    "For example, the lemmatization of “better” is “well” and this another word is derived as lemma as it looks up in the dictionary.\r\n",
    "But the stemming result will come as “better” only without a lookup.\r\n",
    "However, this lookup can at times slow down the lemmatization process.\r\n",
    "\r\n",
    "-Stemming does not take the context of the word into account, for example, “meeting” can be a verb or noun based on the context. \r\n",
    "But lemmatization does consider the context of the word before generating its lemma.\r\n",
    "\r\n",
    "Stemming vs Lemmatization Example\r\n",
    "In the example code below we first tokenize the text and then with the help of for loop stemmed the token with Snowball Stemmer\r\n",
    "and Porter Stemmer. At the same time, we also Lemmatize the text and convert it into a lemma with the help of Wordnet Lemmatizer.\r\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from nltk.stem import SnowballStemmer,PorterStemmer,WordNetLemmatizer\r\n",
    "from nltk import word_tokenize,pos_tag\r\n",
    "\r\n",
    "snowball = SnowballStemmer(language='english')\r\n",
    "porter = PorterStemmer()\r\n",
    "wordnet = WordNetLemmatizer()\r\n",
    "\r\n",
    "text = [\"better\",\"Caring\",\"are\",\"am\",\"worse\",\"struggling\",'meeting']\r\n",
    "print(\"{0:10}{1:20}{2:30}\".format(\"Word\",\"Snowball Stemmer\",\"Wordnet Lemmatizer\"))\r\n",
    "for token,tag in pos_tag(text):\r\n",
    "    \r\n",
    "    pos=tag[0].lower()\r\n",
    "    if pos not in ['a', 'r', 'n', 'v']:\r\n",
    "        pos='n'\r\n",
    "        \r\n",
    "    print(\"{0:10}{1:20}{2:30}\".format(token,snowball.stem(token),wordnet.lemmatize(token,pos)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word      Snowball Stemmer    Wordnet Lemmatizer            \n",
      "better    better              well                          \n",
      "Caring    care                Caring                        \n",
      "are       are                 be                            \n",
      "am        am                  be                            \n",
      "worse     wors                worse                         \n",
      "strugglingstruggl             struggle                      \n",
      "meeting   meet                meeting                       \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('tf-gpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "interpreter": {
   "hash": "135e78ef6267b613ce7b86630936d470174b66187aad9f784a45e5cc3235687c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}