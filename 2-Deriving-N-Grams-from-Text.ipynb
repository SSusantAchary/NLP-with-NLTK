{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Tokenization</h2>\r\n",
    "Splitting a string into substrings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "stng = \"Facebook is an American online social media and social networking\"\r\n",
    "stng = stng.lower()\r\n",
    "stng"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'facebook is an american online social media and social networking'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from nltk.tokenize import RegexpTokenizer\r\n",
    "reg_tokenizer = RegexpTokenizer(\"[a-zA-Z]+\")\r\n",
    "str_tokenized = reg_tokenizer.tokenize(stng)\r\n",
    "str_tokenized"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['facebook',\n",
       " 'is',\n",
       " 'an',\n",
       " 'american',\n",
       " 'online',\n",
       " 'social',\n",
       " 'media',\n",
       " 'and',\n",
       " 'social',\n",
       " 'networking']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>N-Grams (n=4)</h2>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from nltk.util import ngrams\r\n",
    "\r\n",
    "n =4\r\n",
    "generated_n_grams = []\r\n",
    "for word in str_tokenized:\r\n",
    "    generated_n_grams.append(list(ngrams(word, n, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')))\r\n",
    "\r\n",
    "generated_n_grams"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[('_', '_', '_', 'f'),\n",
       "  ('_', '_', 'f', 'a'),\n",
       "  ('_', 'f', 'a', 'c'),\n",
       "  ('f', 'a', 'c', 'e'),\n",
       "  ('a', 'c', 'e', 'b'),\n",
       "  ('c', 'e', 'b', 'o'),\n",
       "  ('e', 'b', 'o', 'o'),\n",
       "  ('b', 'o', 'o', 'k'),\n",
       "  ('o', 'o', 'k', '_'),\n",
       "  ('o', 'k', '_', '_'),\n",
       "  ('k', '_', '_', '_')],\n",
       " [('_', '_', '_', 'i'),\n",
       "  ('_', '_', 'i', 's'),\n",
       "  ('_', 'i', 's', '_'),\n",
       "  ('i', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')],\n",
       " [('_', '_', '_', 'a'),\n",
       "  ('_', '_', 'a', 'n'),\n",
       "  ('_', 'a', 'n', '_'),\n",
       "  ('a', 'n', '_', '_'),\n",
       "  ('n', '_', '_', '_')],\n",
       " [('_', '_', '_', 'a'),\n",
       "  ('_', '_', 'a', 'm'),\n",
       "  ('_', 'a', 'm', 'e'),\n",
       "  ('a', 'm', 'e', 'r'),\n",
       "  ('m', 'e', 'r', 'i'),\n",
       "  ('e', 'r', 'i', 'c'),\n",
       "  ('r', 'i', 'c', 'a'),\n",
       "  ('i', 'c', 'a', 'n'),\n",
       "  ('c', 'a', 'n', '_'),\n",
       "  ('a', 'n', '_', '_'),\n",
       "  ('n', '_', '_', '_')],\n",
       " [('_', '_', '_', 'o'),\n",
       "  ('_', '_', 'o', 'n'),\n",
       "  ('_', 'o', 'n', 'l'),\n",
       "  ('o', 'n', 'l', 'i'),\n",
       "  ('n', 'l', 'i', 'n'),\n",
       "  ('l', 'i', 'n', 'e'),\n",
       "  ('i', 'n', 'e', '_'),\n",
       "  ('n', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 's'),\n",
       "  ('_', '_', 's', 'o'),\n",
       "  ('_', 's', 'o', 'c'),\n",
       "  ('s', 'o', 'c', 'i'),\n",
       "  ('o', 'c', 'i', 'a'),\n",
       "  ('c', 'i', 'a', 'l'),\n",
       "  ('i', 'a', 'l', '_'),\n",
       "  ('a', 'l', '_', '_'),\n",
       "  ('l', '_', '_', '_')],\n",
       " [('_', '_', '_', 'm'),\n",
       "  ('_', '_', 'm', 'e'),\n",
       "  ('_', 'm', 'e', 'd'),\n",
       "  ('m', 'e', 'd', 'i'),\n",
       "  ('e', 'd', 'i', 'a'),\n",
       "  ('d', 'i', 'a', '_'),\n",
       "  ('i', 'a', '_', '_'),\n",
       "  ('a', '_', '_', '_')],\n",
       " [('_', '_', '_', 'a'),\n",
       "  ('_', '_', 'a', 'n'),\n",
       "  ('_', 'a', 'n', 'd'),\n",
       "  ('a', 'n', 'd', '_'),\n",
       "  ('n', 'd', '_', '_'),\n",
       "  ('d', '_', '_', '_')],\n",
       " [('_', '_', '_', 's'),\n",
       "  ('_', '_', 's', 'o'),\n",
       "  ('_', 's', 'o', 'c'),\n",
       "  ('s', 'o', 'c', 'i'),\n",
       "  ('o', 'c', 'i', 'a'),\n",
       "  ('c', 'i', 'a', 'l'),\n",
       "  ('i', 'a', 'l', '_'),\n",
       "  ('a', 'l', '_', '_'),\n",
       "  ('l', '_', '_', '_')],\n",
       " [('_', '_', '_', 'n'),\n",
       "  ('_', '_', 'n', 'e'),\n",
       "  ('_', 'n', 'e', 't'),\n",
       "  ('n', 'e', 't', 'w'),\n",
       "  ('e', 't', 'w', 'o'),\n",
       "  ('t', 'w', 'o', 'r'),\n",
       "  ('w', 'o', 'r', 'k'),\n",
       "  ('o', 'r', 'k', 'i'),\n",
       "  ('r', 'k', 'i', 'n'),\n",
       "  ('k', 'i', 'n', 'g'),\n",
       "  ('i', 'n', 'g', '_'),\n",
       "  ('n', 'g', '_', '_'),\n",
       "  ('g', '_', '_', '_')]]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>flatten this to just list 4-grams</h2>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "flat_list_of_ngrams = []\r\n",
    "for sublist in generated_n_grams:\r\n",
    "    for item in sublist:\r\n",
    "        flat_list_of_ngrams.append(item)\r\n",
    "\r\n",
    "flat_list_of_ngrams"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('_', '_', '_', 'f'),\n",
       " ('_', '_', 'f', 'a'),\n",
       " ('_', 'f', 'a', 'c'),\n",
       " ('f', 'a', 'c', 'e'),\n",
       " ('a', 'c', 'e', 'b'),\n",
       " ('c', 'e', 'b', 'o'),\n",
       " ('e', 'b', 'o', 'o'),\n",
       " ('b', 'o', 'o', 'k'),\n",
       " ('o', 'o', 'k', '_'),\n",
       " ('o', 'k', '_', '_'),\n",
       " ('k', '_', '_', '_'),\n",
       " ('_', '_', '_', 'i'),\n",
       " ('_', '_', 'i', 's'),\n",
       " ('_', 'i', 's', '_'),\n",
       " ('i', 's', '_', '_'),\n",
       " ('s', '_', '_', '_'),\n",
       " ('_', '_', '_', 'a'),\n",
       " ('_', '_', 'a', 'n'),\n",
       " ('_', 'a', 'n', '_'),\n",
       " ('a', 'n', '_', '_'),\n",
       " ('n', '_', '_', '_'),\n",
       " ('_', '_', '_', 'a'),\n",
       " ('_', '_', 'a', 'm'),\n",
       " ('_', 'a', 'm', 'e'),\n",
       " ('a', 'm', 'e', 'r'),\n",
       " ('m', 'e', 'r', 'i'),\n",
       " ('e', 'r', 'i', 'c'),\n",
       " ('r', 'i', 'c', 'a'),\n",
       " ('i', 'c', 'a', 'n'),\n",
       " ('c', 'a', 'n', '_'),\n",
       " ('a', 'n', '_', '_'),\n",
       " ('n', '_', '_', '_'),\n",
       " ('_', '_', '_', 'o'),\n",
       " ('_', '_', 'o', 'n'),\n",
       " ('_', 'o', 'n', 'l'),\n",
       " ('o', 'n', 'l', 'i'),\n",
       " ('n', 'l', 'i', 'n'),\n",
       " ('l', 'i', 'n', 'e'),\n",
       " ('i', 'n', 'e', '_'),\n",
       " ('n', 'e', '_', '_'),\n",
       " ('e', '_', '_', '_'),\n",
       " ('_', '_', '_', 's'),\n",
       " ('_', '_', 's', 'o'),\n",
       " ('_', 's', 'o', 'c'),\n",
       " ('s', 'o', 'c', 'i'),\n",
       " ('o', 'c', 'i', 'a'),\n",
       " ('c', 'i', 'a', 'l'),\n",
       " ('i', 'a', 'l', '_'),\n",
       " ('a', 'l', '_', '_'),\n",
       " ('l', '_', '_', '_'),\n",
       " ('_', '_', '_', 'm'),\n",
       " ('_', '_', 'm', 'e'),\n",
       " ('_', 'm', 'e', 'd'),\n",
       " ('m', 'e', 'd', 'i'),\n",
       " ('e', 'd', 'i', 'a'),\n",
       " ('d', 'i', 'a', '_'),\n",
       " ('i', 'a', '_', '_'),\n",
       " ('a', '_', '_', '_'),\n",
       " ('_', '_', '_', 'a'),\n",
       " ('_', '_', 'a', 'n'),\n",
       " ('_', 'a', 'n', 'd'),\n",
       " ('a', 'n', 'd', '_'),\n",
       " ('n', 'd', '_', '_'),\n",
       " ('d', '_', '_', '_'),\n",
       " ('_', '_', '_', 's'),\n",
       " ('_', '_', 's', 'o'),\n",
       " ('_', 's', 'o', 'c'),\n",
       " ('s', 'o', 'c', 'i'),\n",
       " ('o', 'c', 'i', 'a'),\n",
       " ('c', 'i', 'a', 'l'),\n",
       " ('i', 'a', 'l', '_'),\n",
       " ('a', 'l', '_', '_'),\n",
       " ('l', '_', '_', '_'),\n",
       " ('_', '_', '_', 'n'),\n",
       " ('_', '_', 'n', 'e'),\n",
       " ('_', 'n', 'e', 't'),\n",
       " ('n', 'e', 't', 'w'),\n",
       " ('e', 't', 'w', 'o'),\n",
       " ('t', 'w', 'o', 'r'),\n",
       " ('w', 'o', 'r', 'k'),\n",
       " ('o', 'r', 'k', 'i'),\n",
       " ('r', 'k', 'i', 'n'),\n",
       " ('k', 'i', 'n', 'g'),\n",
       " ('i', 'n', 'g', '_'),\n",
       " ('n', 'g', '_', '_'),\n",
       " ('g', '_', '_', '_')]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Now join the elements within a tuple to get just one list of n-grams</h2>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def convertTuple(tupl):\r\n",
    "    str_ng = ''.join(tupl)\r\n",
    "    return str_ng"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "list_of_quadgrams = []\r\n",
    "\r\n",
    "for line in flat_list_of_ngrams:\r\n",
    "    list_of_quadgrams.append(convertTuple(line))\r\n",
    "list_of_quadgrams"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['___f',\n",
       " '__fa',\n",
       " '_fac',\n",
       " 'face',\n",
       " 'aceb',\n",
       " 'cebo',\n",
       " 'eboo',\n",
       " 'book',\n",
       " 'ook_',\n",
       " 'ok__',\n",
       " 'k___',\n",
       " '___i',\n",
       " '__is',\n",
       " '_is_',\n",
       " 'is__',\n",
       " 's___',\n",
       " '___a',\n",
       " '__an',\n",
       " '_an_',\n",
       " 'an__',\n",
       " 'n___',\n",
       " '___a',\n",
       " '__am',\n",
       " '_ame',\n",
       " 'amer',\n",
       " 'meri',\n",
       " 'eric',\n",
       " 'rica',\n",
       " 'ican',\n",
       " 'can_',\n",
       " 'an__',\n",
       " 'n___',\n",
       " '___o',\n",
       " '__on',\n",
       " '_onl',\n",
       " 'onli',\n",
       " 'nlin',\n",
       " 'line',\n",
       " 'ine_',\n",
       " 'ne__',\n",
       " 'e___',\n",
       " '___s',\n",
       " '__so',\n",
       " '_soc',\n",
       " 'soci',\n",
       " 'ocia',\n",
       " 'cial',\n",
       " 'ial_',\n",
       " 'al__',\n",
       " 'l___',\n",
       " '___m',\n",
       " '__me',\n",
       " '_med',\n",
       " 'medi',\n",
       " 'edia',\n",
       " 'dia_',\n",
       " 'ia__',\n",
       " 'a___',\n",
       " '___a',\n",
       " '__an',\n",
       " '_and',\n",
       " 'and_',\n",
       " 'nd__',\n",
       " 'd___',\n",
       " '___s',\n",
       " '__so',\n",
       " '_soc',\n",
       " 'soci',\n",
       " 'ocia',\n",
       " 'cial',\n",
       " 'ial_',\n",
       " 'al__',\n",
       " 'l___',\n",
       " '___n',\n",
       " '__ne',\n",
       " '_net',\n",
       " 'netw',\n",
       " 'etwo',\n",
       " 'twor',\n",
       " 'work',\n",
       " 'orki',\n",
       " 'rkin',\n",
       " 'king',\n",
       " 'ing_',\n",
       " 'ng__',\n",
       " 'g___']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Sorting the n-grams in decreasing order of frequency</h2>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "freq_n_grams = {}\r\n",
    "\r\n",
    "for ngram in list_of_quadgrams:\r\n",
    "    if ngram not in freq_n_grams:\r\n",
    "        freq_n_grams.update({ngram: 1})\r\n",
    "    else:\r\n",
    "        ngram_occurances = freq_n_grams[ngram]\r\n",
    "        freq_n_grams.update({ngram: ngram_occurances + 1})\r\n",
    "\r\n",
    "freq_n_grams"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'___f': 1,\n",
       " '__fa': 1,\n",
       " '_fac': 1,\n",
       " 'face': 1,\n",
       " 'aceb': 1,\n",
       " 'cebo': 1,\n",
       " 'eboo': 1,\n",
       " 'book': 1,\n",
       " 'ook_': 1,\n",
       " 'ok__': 1,\n",
       " 'k___': 1,\n",
       " '___i': 1,\n",
       " '__is': 1,\n",
       " '_is_': 1,\n",
       " 'is__': 1,\n",
       " 's___': 1,\n",
       " '___a': 3,\n",
       " '__an': 2,\n",
       " '_an_': 1,\n",
       " 'an__': 2,\n",
       " 'n___': 2,\n",
       " '__am': 1,\n",
       " '_ame': 1,\n",
       " 'amer': 1,\n",
       " 'meri': 1,\n",
       " 'eric': 1,\n",
       " 'rica': 1,\n",
       " 'ican': 1,\n",
       " 'can_': 1,\n",
       " '___o': 1,\n",
       " '__on': 1,\n",
       " '_onl': 1,\n",
       " 'onli': 1,\n",
       " 'nlin': 1,\n",
       " 'line': 1,\n",
       " 'ine_': 1,\n",
       " 'ne__': 1,\n",
       " 'e___': 1,\n",
       " '___s': 2,\n",
       " '__so': 2,\n",
       " '_soc': 2,\n",
       " 'soci': 2,\n",
       " 'ocia': 2,\n",
       " 'cial': 2,\n",
       " 'ial_': 2,\n",
       " 'al__': 2,\n",
       " 'l___': 2,\n",
       " '___m': 1,\n",
       " '__me': 1,\n",
       " '_med': 1,\n",
       " 'medi': 1,\n",
       " 'edia': 1,\n",
       " 'dia_': 1,\n",
       " 'ia__': 1,\n",
       " 'a___': 1,\n",
       " '_and': 1,\n",
       " 'and_': 1,\n",
       " 'nd__': 1,\n",
       " 'd___': 1,\n",
       " '___n': 1,\n",
       " '__ne': 1,\n",
       " '_net': 1,\n",
       " 'netw': 1,\n",
       " 'etwo': 1,\n",
       " 'twor': 1,\n",
       " 'work': 1,\n",
       " 'orki': 1,\n",
       " 'rkin': 1,\n",
       " 'king': 1,\n",
       " 'ing_': 1,\n",
       " 'ng__': 1,\n",
       " 'g___': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from operator import itemgetter\r\n",
    "\r\n",
    "freq_n_grams_sorted = sorted(freq_n_grams.items(), key=itemgetter(1), reverse=True)[0:300]\r\n",
    "\r\n",
    "freq_n_grams_sorted"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('___a', 3),\n",
       " ('__an', 2),\n",
       " ('an__', 2),\n",
       " ('n___', 2),\n",
       " ('___s', 2),\n",
       " ('__so', 2),\n",
       " ('_soc', 2),\n",
       " ('soci', 2),\n",
       " ('ocia', 2),\n",
       " ('cial', 2),\n",
       " ('ial_', 2),\n",
       " ('al__', 2),\n",
       " ('l___', 2),\n",
       " ('___f', 1),\n",
       " ('__fa', 1),\n",
       " ('_fac', 1),\n",
       " ('face', 1),\n",
       " ('aceb', 1),\n",
       " ('cebo', 1),\n",
       " ('eboo', 1),\n",
       " ('book', 1),\n",
       " ('ook_', 1),\n",
       " ('ok__', 1),\n",
       " ('k___', 1),\n",
       " ('___i', 1),\n",
       " ('__is', 1),\n",
       " ('_is_', 1),\n",
       " ('is__', 1),\n",
       " ('s___', 1),\n",
       " ('_an_', 1),\n",
       " ('__am', 1),\n",
       " ('_ame', 1),\n",
       " ('amer', 1),\n",
       " ('meri', 1),\n",
       " ('eric', 1),\n",
       " ('rica', 1),\n",
       " ('ican', 1),\n",
       " ('can_', 1),\n",
       " ('___o', 1),\n",
       " ('__on', 1),\n",
       " ('_onl', 1),\n",
       " ('onli', 1),\n",
       " ('nlin', 1),\n",
       " ('line', 1),\n",
       " ('ine_', 1),\n",
       " ('ne__', 1),\n",
       " ('e___', 1),\n",
       " ('___m', 1),\n",
       " ('__me', 1),\n",
       " ('_med', 1),\n",
       " ('medi', 1),\n",
       " ('edia', 1),\n",
       " ('dia_', 1),\n",
       " ('ia__', 1),\n",
       " ('a___', 1),\n",
       " ('_and', 1),\n",
       " ('and_', 1),\n",
       " ('nd__', 1),\n",
       " ('d___', 1),\n",
       " ('___n', 1),\n",
       " ('__ne', 1),\n",
       " ('_net', 1),\n",
       " ('netw', 1),\n",
       " ('etwo', 1),\n",
       " ('twor', 1),\n",
       " ('work', 1),\n",
       " ('orki', 1),\n",
       " ('rkin', 1),\n",
       " ('king', 1),\n",
       " ('ing_', 1),\n",
       " ('ng__', 1),\n",
       " ('g___', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>N-grams for multiple value of n</h2>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "str_clean = ' '.join(str_tokenized)\r\n",
    "str_clean"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'facebook is an american online social media and social networking'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from nltk import everygrams\r\n",
    "\r\n",
    "def ngram_extractor(sent):\r\n",
    "    return [''.join(ng) for ng in everygrams(sent.replace(' ','_ _'),1, 4)\r\n",
    "           if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\r\n",
    "\r\n",
    "ngram_extractor(str_clean)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['f',\n",
       " 'a',\n",
       " 'c',\n",
       " 'e',\n",
       " 'b',\n",
       " 'o',\n",
       " 'o',\n",
       " 'k',\n",
       " 'i',\n",
       " 's',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'm',\n",
       " 'e',\n",
       " 'r',\n",
       " 'i',\n",
       " 'c',\n",
       " 'a',\n",
       " 'n',\n",
       " 'o',\n",
       " 'n',\n",
       " 'l',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e',\n",
       " 's',\n",
       " 'o',\n",
       " 'c',\n",
       " 'i',\n",
       " 'a',\n",
       " 'l',\n",
       " 'm',\n",
       " 'e',\n",
       " 'd',\n",
       " 'i',\n",
       " 'a',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " 's',\n",
       " 'o',\n",
       " 'c',\n",
       " 'i',\n",
       " 'a',\n",
       " 'l',\n",
       " 'n',\n",
       " 'e',\n",
       " 't',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'k',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " 'fa',\n",
       " 'ac',\n",
       " 'ce',\n",
       " 'eb',\n",
       " 'bo',\n",
       " 'oo',\n",
       " 'ok',\n",
       " 'k_',\n",
       " '_i',\n",
       " 'is',\n",
       " 's_',\n",
       " '_a',\n",
       " 'an',\n",
       " 'n_',\n",
       " '_a',\n",
       " 'am',\n",
       " 'me',\n",
       " 'er',\n",
       " 'ri',\n",
       " 'ic',\n",
       " 'ca',\n",
       " 'an',\n",
       " 'n_',\n",
       " '_o',\n",
       " 'on',\n",
       " 'nl',\n",
       " 'li',\n",
       " 'in',\n",
       " 'ne',\n",
       " 'e_',\n",
       " '_s',\n",
       " 'so',\n",
       " 'oc',\n",
       " 'ci',\n",
       " 'ia',\n",
       " 'al',\n",
       " 'l_',\n",
       " '_m',\n",
       " 'me',\n",
       " 'ed',\n",
       " 'di',\n",
       " 'ia',\n",
       " 'a_',\n",
       " '_a',\n",
       " 'an',\n",
       " 'nd',\n",
       " 'd_',\n",
       " '_s',\n",
       " 'so',\n",
       " 'oc',\n",
       " 'ci',\n",
       " 'ia',\n",
       " 'al',\n",
       " 'l_',\n",
       " '_n',\n",
       " 'ne',\n",
       " 'et',\n",
       " 'tw',\n",
       " 'wo',\n",
       " 'or',\n",
       " 'rk',\n",
       " 'ki',\n",
       " 'in',\n",
       " 'ng',\n",
       " 'fac',\n",
       " 'ace',\n",
       " 'ceb',\n",
       " 'ebo',\n",
       " 'boo',\n",
       " 'ook',\n",
       " 'ok_',\n",
       " '_is',\n",
       " 'is_',\n",
       " '_an',\n",
       " 'an_',\n",
       " '_am',\n",
       " 'ame',\n",
       " 'mer',\n",
       " 'eri',\n",
       " 'ric',\n",
       " 'ica',\n",
       " 'can',\n",
       " 'an_',\n",
       " '_on',\n",
       " 'onl',\n",
       " 'nli',\n",
       " 'lin',\n",
       " 'ine',\n",
       " 'ne_',\n",
       " '_so',\n",
       " 'soc',\n",
       " 'oci',\n",
       " 'cia',\n",
       " 'ial',\n",
       " 'al_',\n",
       " '_me',\n",
       " 'med',\n",
       " 'edi',\n",
       " 'dia',\n",
       " 'ia_',\n",
       " '_an',\n",
       " 'and',\n",
       " 'nd_',\n",
       " '_so',\n",
       " 'soc',\n",
       " 'oci',\n",
       " 'cia',\n",
       " 'ial',\n",
       " 'al_',\n",
       " '_ne',\n",
       " 'net',\n",
       " 'etw',\n",
       " 'two',\n",
       " 'wor',\n",
       " 'ork',\n",
       " 'rki',\n",
       " 'kin',\n",
       " 'ing',\n",
       " 'face',\n",
       " 'aceb',\n",
       " 'cebo',\n",
       " 'eboo',\n",
       " 'book',\n",
       " 'ook_',\n",
       " '_is_',\n",
       " '_an_',\n",
       " '_ame',\n",
       " 'amer',\n",
       " 'meri',\n",
       " 'eric',\n",
       " 'rica',\n",
       " 'ican',\n",
       " 'can_',\n",
       " '_onl',\n",
       " 'onli',\n",
       " 'nlin',\n",
       " 'line',\n",
       " 'ine_',\n",
       " '_soc',\n",
       " 'soci',\n",
       " 'ocia',\n",
       " 'cial',\n",
       " 'ial_',\n",
       " '_med',\n",
       " 'medi',\n",
       " 'edia',\n",
       " 'dia_',\n",
       " '_and',\n",
       " 'and_',\n",
       " '_soc',\n",
       " 'soci',\n",
       " 'ocia',\n",
       " 'cial',\n",
       " 'ial_',\n",
       " '_net',\n",
       " 'netw',\n",
       " 'etwo',\n",
       " 'twor',\n",
       " 'work',\n",
       " 'orki',\n",
       " 'rkin',\n",
       " 'king']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Identify the Language</h2>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import nltk\r\n",
    "nltk.download('crubadan')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package crubadan to\n",
      "[nltk_data]     C:\\Users\\susan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\crubadan.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from nltk.classify import textcat\r\n",
    "\r\n",
    "tc_class = textcat.TextCat()\r\n",
    "\r\n",
    "tc_class.guess_language(stng)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "tc_class.guess_language(\"हेलो आई वर्क इन फेसबुक\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'hin'"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "135e78ef6267b613ce7b86630936d470174b66187aad9f784a45e5cc3235687c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}